```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
```

### 1. Label Encoding 
embark_town 변수를 레이블 인코딩하고,  <p>
레이블 인코딩한 값을 기존 데이터 프레임에 'embark_town_labels'라는 열로 추가해보세요.<p>


```python
from sklearn.preprocessing import LabelEncoder
titanic = sns.load_dataset('titanic')
```


```python
le = LabelEncoder()
titanic['embark_town_labels']=le.fit_transform(titanic['embark_town'])
```


```python
titanic
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>survived</th>
      <th>pclass</th>
      <th>sex</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>fare</th>
      <th>embarked</th>
      <th>class</th>
      <th>who</th>
      <th>adult_male</th>
      <th>deck</th>
      <th>embark_town</th>
      <th>alive</th>
      <th>alone</th>
      <th>embark_town_labels</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>S</td>
      <td>Third</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>no</td>
      <td>False</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>C</td>
      <td>First</td>
      <td>woman</td>
      <td>False</td>
      <td>C</td>
      <td>Cherbourg</td>
      <td>yes</td>
      <td>False</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
      <td>S</td>
      <td>Third</td>
      <td>woman</td>
      <td>False</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>yes</td>
      <td>True</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>53.1000</td>
      <td>S</td>
      <td>First</td>
      <td>woman</td>
      <td>False</td>
      <td>C</td>
      <td>Southampton</td>
      <td>yes</td>
      <td>False</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>8.0500</td>
      <td>S</td>
      <td>Third</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>no</td>
      <td>True</td>
      <td>2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>886</th>
      <td>0</td>
      <td>2</td>
      <td>male</td>
      <td>27.0</td>
      <td>0</td>
      <td>0</td>
      <td>13.0000</td>
      <td>S</td>
      <td>Second</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>no</td>
      <td>True</td>
      <td>2</td>
    </tr>
    <tr>
      <th>887</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>19.0</td>
      <td>0</td>
      <td>0</td>
      <td>30.0000</td>
      <td>S</td>
      <td>First</td>
      <td>woman</td>
      <td>False</td>
      <td>B</td>
      <td>Southampton</td>
      <td>yes</td>
      <td>True</td>
      <td>2</td>
    </tr>
    <tr>
      <th>888</th>
      <td>0</td>
      <td>3</td>
      <td>female</td>
      <td>NaN</td>
      <td>1</td>
      <td>2</td>
      <td>23.4500</td>
      <td>S</td>
      <td>Third</td>
      <td>woman</td>
      <td>False</td>
      <td>NaN</td>
      <td>Southampton</td>
      <td>no</td>
      <td>False</td>
      <td>2</td>
    </tr>
    <tr>
      <th>889</th>
      <td>1</td>
      <td>1</td>
      <td>male</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>30.0000</td>
      <td>C</td>
      <td>First</td>
      <td>man</td>
      <td>True</td>
      <td>C</td>
      <td>Cherbourg</td>
      <td>yes</td>
      <td>True</td>
      <td>0</td>
    </tr>
    <tr>
      <th>890</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>32.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.7500</td>
      <td>Q</td>
      <td>Third</td>
      <td>man</td>
      <td>True</td>
      <td>NaN</td>
      <td>Queenstown</td>
      <td>no</td>
      <td>True</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>891 rows × 16 columns</p>
</div>



### 2. 결측값/중복값 처리 
타민은 한국환경공단에서 발표한 미세먼지 관련 자료를 분석하려고 데이터를 다운로드받았습니다. <p>
하지만 데이터에 결측값이 많아 이를 우선 조정하려고 합니다. <p>
    타민과 함께 데이터를 처리해주세요! ('finedust.csv' 파일을 활용하세요)

#### 2.1. 
파일을 불러와 컬럼 별 결측치 개수를 구하세요. <p>
(이 파일은 공공기관 파일로, 인코딩이 utf-8이 아닌 cp949로 되어있습니다! <p>
    read_csv 사용 시 오류가 발생하면 pd.read_csv('경로\\finedust.csv', engine='python')으로 불러와주세요! <p>
        기타 문제 발생 시 바로 문의주세요!)


```python
df = pd.read_csv('finedust.csv',encoding = 'euc-kr');df.isnull().sum()
```




    측정일자            3
    측정시간            8
    지역              9
    지역명             9
    도로명           194
    시작점           151
    종점             63
    기온             12
    습도             32
    재비산먼지 평균농도      4
    오염범례           27
    dtype: int64



#### 2.2
타민은 '분석할만한 데이터가 6개 미만인 데이터는 제외해도 좋을 것 같은데?'라며 해당 데이터를 삭제하려 합니다.<p>
적합한 코드를 알려주세요. (tip: dropna, tresh 활용)


```python
df = df.dropna(axis=0, thresh=6)
```

#### 2.3
데이터를 살펴보던 타민은 아래와 같이 데이터를 조작해서 다음 분석 단계로 넘어가려 합니다. <p>아래 순서와 조건에 맞게 데이터를 수정해보세요(inplace=True).

    1) '도로명', '시작점', '종점' 데이터를 제외하고 분석할거야 (컬럼 기준 tresh 사용하기)
    2) '측정 시간', '지역', '지역명'의 공백은 바로 위 데이터로 대체할래 (hint: method='ffill')
    3) '측정 일자'와 '지역명', '재비산먼지'가 중복되는 데이터는 가장 첫번째 케이스를 제외하고 제외해야지 (hint:drop_duplicates, keep='First')
    4) '기온', '습도'의 결측값은 각 열의 평균값으로 채울래
    5) '오염범례'의 결측값은 최빈값으로 대체하자(hint: value_counts())


```python
df.dropna(axis=1, thresh=250,inplace = True)
```


```python
df[['측정시간','지역','지역명']].fillna(method='ffill')
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>측정시간</th>
      <th>지역</th>
      <th>지역명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9:44</td>
      <td>서울</td>
      <td>구로구</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10:04</td>
      <td>서울</td>
      <td>구로구</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10:28</td>
      <td>서울</td>
      <td>구로구</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10:35</td>
      <td>서울</td>
      <td>구로구</td>
    </tr>
    <tr>
      <th>4</th>
      <td>11:04</td>
      <td>서울</td>
      <td>구로구</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>295</th>
      <td>13:42</td>
      <td>경기</td>
      <td>화성시</td>
    </tr>
    <tr>
      <th>296</th>
      <td>13:55</td>
      <td>경기</td>
      <td>화성시</td>
    </tr>
    <tr>
      <th>297</th>
      <td>14:17</td>
      <td>경기</td>
      <td>화성시</td>
    </tr>
    <tr>
      <th>298</th>
      <td>14:27</td>
      <td>경기</td>
      <td>화성시</td>
    </tr>
    <tr>
      <th>299</th>
      <td>14:36</td>
      <td>경기</td>
      <td>화성시</td>
    </tr>
  </tbody>
</table>
<p>297 rows × 3 columns</p>
</div>




```python
df = df.drop_duplicates(subset = ['측정일자','지역명','재비산먼지 평균농도'],keep = 'first')
```


```python
df[['기온','습도']] = df[['기온','습도']].replace(to_replace = np.nan, value=df[['기온','습도']].mean())
```


```python
df['오염범례'].value_counts()
```




    매우좋음    176
    좋음       40
    보통        9
    나쁨        5
    매우나쁨      2
    Name: 오염범례, dtype: int64




```python
df['오염범례'] = df['오염범례'].fillna('매우좋음')
```


```python
df.isnull().sum()
```




    측정일자          0
    측정시간          3
    지역            3
    지역명           4
    기온            0
    습도            0
    재비산먼지 평균농도    0
    오염범례          0
    dtype: int64



#### 2.4 
타민처럼 데이터를 처리했을 때 생길 수 있는 문제점은 무엇일까요? 여러분이라면 이 데이터를 어떻게 처리할 것인가요? <p>
    자유롭게 생각해봅시다:)


```python

```

### 3. 불균형 데이터
1번 문제에서 만든 embark_town_labels 열을 추가한 titanic 데이터프레임을 사용하여,  <p>
X는 pclass, embark_town_labels, y는 survived로 X와 y값을 설정하세요.  <p>
위 X,y에 대해 over sampling 기법 중 하나를 선택하여 전처리를 진행하고 어떻게 데이터가 변화하였는지 살펴보세요.  <p>
(hint: shape, value_counts 등을 통해 샘플링 전후 데이터 개수의 변화를 알 수 있습니다.)<p>


```python
from imblearn.over_sampling import *
```


```python
X = titanic[['pclass','embark_town_labels']]
y = titanic['survived']
```


```python
X.shape, y.shape
```




    ((891, 2), (891,))




```python
X_resampled, y_resampled = RandomOverSampler(random_state=0).fit_resample(X,y)
```


```python
X_resampled.shape, y_resampled.shape
```




    ((1098, 2), (1098,))



### 4. Feature Scaling
학구열로 불타오르는 타민이는 정규 세션에서 배운 Feature Scaling 내용을 스스로 공부해보려고 한다.<p>
최근 영화에 관심에 생긴 타민이는 네이버 영화와 왓챠를 자주 이용한다.<p>
영화 평점에 굉장히 민감한 타민이는 최근 유행하는 영화 50편의 평점 데이터를 네이버와 왓챠피디아에서 가져왔고, <p>
이를 통해 두 영화 평점 사이트의 데이터를 비교해보고자 한다.<p>
고민에 빠진 타민이를 도와주자!


```python
from sklearn.preprocessing import MinMaxScaler

naver_data = pd.read_csv('naver_movie_rating.csv')
watcha_data = pd.read_csv('watcha_movie_rating.csv')

# 완성되지 않은 부분을 채우세요!

# naver_data에서 movie_rating 컬럼만 추출하여 1차원 numpy array로 변환해준다.
naver_rating = naver_data['movie_rating'].to_numpy()

# watcha_data에서 movie_rating 컬럼만 추출하여 1차원 numpy array로 변환해준다.
watcha_rating = watcha_data['movie_rating'].to_numpy()
```

<p style="line-height:200%">네이버에서는 영화 평점을 <b>최소 0점부터 최대 10점까지</b> 부여할 수 있고, 왓챠에서는 영화 평점을 <b>최소 0점부터 최대 5점까지</b> 부여할 수 있다. 즉, 네이버와 왓챠의 평점은 <b>각각 10점, 5점이 만점</b>이고, <b>최저점으로 0점</b>이 존재할 수 있다.</p>
<br>
<p style="line-height:200%">이를 고려하여 각 사이트의 평점 데이터를 <b>0부터 1까지의 범위</b>로 스케일링한 결과를 구해보자.</p>
<br>
<p style="line-height:200%"><b>※ MinMaxScaler 사용</b></p>


```python
naver_rating
```




    array([8.64, 6.  , 7.62, 7.49, 9.11, 8.53, 8.4 , 8.1 , 8.47, 7.11, 7.52,
           8.58, 7.01, 8.11, 8.32, 9.32, 9.17, 8.98, 7.91, 9.11, 8.55, 8.48,
           5.67, 6.86, 7.84, 8.93, 6.79, 6.97, 9.32, 8.17, 6.72, 8.25, 9.23,
           8.96, 8.91, 8.31, 7.37, 8.91, 8.85, 8.35, 8.59, 7.57, 9.23, 7.18,
           9.12, 8.58, 8.1 , 9.19, 7.4 , 8.77])




```python
# 완성되지 않은 부분을 채우세요!

## 미완성 부분 ##
from sklearn.preprocessing import MinMaxScaler
naver_rating_scaled = MinMaxScaler().fit_transform(naver_rating.reshape(-1,1))

print('스케일링 된 네이버 영화 평점 데이터')
print('최저 평점: ', np.round(naver_rating_scaled.min(),2))
print('최고 평점: ', np.round(naver_rating_scaled.max(),2))
print('평균 평점: ', np.round(naver_rating_scaled.mean(),2))
print('평점 분산: ', np.round(naver_rating_scaled.var(),2))
```

    스케일링 된 네이버 영화 평점 데이터
    최저 평점:  0.0
    최고 평점:  1.0
    평균 평점:  0.69
    평점 분산:  0.06
    

<p style="line-height:200%"><b>아래의 결과처럼 나오도록 한다.</b></p>
<br>
스케일링 된 네이버 영화 평점 데이터<br>
최저 평점:  0.57<br>
최고 평점:  0.93<br>
평균 평점:  0.82<br>
평점 분산:  0.01<br>


```python
# 완성되지 않은 부분을 채우세요!

## 미완성 부분 ##

watcha_rating_scaled = MinMaxScaler().fit_transform(watcha_rating.reshape(-1,1))

print('스케일링 된 왓챠 영화 평점 데이터')
print('최저 평점: ', np.round(watcha_rating_scaled.min(),2))
print('최고 평점: ', np.round(watcha_rating_scaled.max(),2))
print('평균 평점: ', np.round(watcha_rating_scaled.mean(),2))
print('평점 분산: ', np.round(watcha_rating_scaled.var(),2))
```

    스케일링 된 왓챠 영화 평점 데이터
    최저 평점:  0.0
    최고 평점:  1.0
    평균 평점:  0.62
    평점 분산:  0.06
    

<p style="line-height:200%"><b>아래의 결과처럼 나오도록 한다.</b></p>
<br>
스케일링 된 왓챠 영화 평점 데이터<br>
최저 평점:  0.5<br>
최고 평점:  0.84<br>
평균 평점:  0.71<br>
평점 분산:  0.01<br>

<p style="line-height:200%">만약 네이버와 왓챠 평점의 <b>만점이 모두 10점</b>이고, 조사한 왓챠 평점 데이터가 5점 만점이 아닌 <b>10점 만점일 때의 데이터</b>라고 가정해 보자.</p>
<br>
<p>이 때 왓챠 사이트의 평점 데이터를 <b>0부터 1까지의 범위</b>로 스케일링한 결과를 각각 구해보자.</p>


```python
# 완성되지 않은 부분을 채우세요!

## 미완성 부분 ##

watcha_rating_scaled = MinMaxScaler().fit_transform(watcha_rating.reshape(-1,1))

print('스케일링 된 10점 만점의 왓챠 영화 평점 데이터')
print('최저 평점: ', np.round(watcha_rating_scaled.min(),2))
print('최고 평점: ', np.round(watcha_rating_scaled.max(),2))
print('평균 평점: ', np.round(watcha_rating_scaled.mean(),2))
print('평점 분산: ', np.round(watcha_rating_scaled.var(),2))
```

    스케일링 된 10점 만점의 왓챠 영화 평점 데이터
    최저 평점:  0.0
    최고 평점:  1.0
    평균 평점:  0.62
    평점 분산:  0.06
    

#### 마지막 문제 이해가 잘 안되네요..
###### 그리고 minmax 정규화는 수식 상 항상 최저가 0이고 최고가 1이 되어야 하는 것 아닌가요? 그냥 여기서는 최저가 0점이고 최고점이 10점으로 가정하고 계산하는 것인가요?... 감사합니다!


```python

```
